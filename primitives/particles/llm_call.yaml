metadata:
  id: "P007"
  name: "llm_call"
  type: "particle"
  version: "1.0.0"
  status: "stable"
  description: "Invoke an LLM model for text generation, classification, or extraction"
  category: "ai"
  tags:
    - llm
    - ai
    - gpt
    - claude
    - gemini
    - completion

interface:
  inputs:
    - name: "prompt"
      type: "string"
      required: true
      description: "The prompt to send to the model"
    - name: "model"
      type: "enum"
      enum_values: ["orchestrator", "validator", "local", "claude-sonnet-4-5", "gpt-4o", "gemini-pro"]
      required: false
      default: "orchestrator"
      description: "Model tier or specific model name"
    - name: "system_prompt"
      type: "string"
      required: false
      description: "System message for context"
    - name: "output_schema"
      type: "object"
      required: false
      description: "JSON Schema for structured output"
    - name: "temperature"
      type: "number"
      required: false
      default: 0.7
      description: "Sampling temperature (0-2)"
    - name: "max_tokens"
      type: "number"
      required: false
      default: 1000
      description: "Maximum tokens to generate"
    - name: "stop"
      type: "array"
      required: false
      description: "Stop sequences"
    - name: "tools"
      type: "array"
      required: false
      description: "Available tools/functions for the model"

  outputs:
    - name: "response"
      type: "any"
      description: "Model response (string or structured object)"
    - name: "usage"
      type: "object"
      description: "Token usage stats"
    - name: "tool_calls"
      type: "array"
      description: "Tool calls made by the model"
    - name: "finish_reason"
      type: "string"
      description: "Why generation stopped"

  errors:
    - code: "RATE_LIMIT"
      description: "API rate limit exceeded"
      retryable: true
    - code: "CONTEXT_LENGTH"
      description: "Input exceeds model context limit"
      retryable: false
    - code: "INVALID_SCHEMA"
      description: "Output did not match schema"
      retryable: true
    - code: "API_ERROR"
      description: "Provider API error"
      retryable: true

compilation_targets:
  n8n:
    node_type: "openAi"
    version: "1.5"
  python:
    module: "litellm"
    function: "completion"
  temporal:
    activity: "llm_call_activity"

constraints:
  timeout: "120s"
  retry_count: 3
  idempotent: false  # Non-deterministic

examples:
  - name: "Extract invoice data"
    inputs:
      prompt: |
        Extract the following from this invoice:
        - Invoice number
        - Total amount
        - Due date
        - Vendor name

        Invoice text:
        {{ ref: fetch_email.body.content }}
      model: "orchestrator"
      output_schema:
        type: object
        properties:
          invoice_number:
            type: string
          amount:
            type: number
          due_date:
            type: string
            format: date
          vendor:
            type: string
        required: ["invoice_number", "amount"]

  - name: "Classify intent"
    inputs:
      prompt: "Classify this customer message: {{ input.message }}"
      system_prompt: "You are a customer service classifier. Return one of: billing, technical, general, complaint"
      model: "validator"
      temperature: 0
